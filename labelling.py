# -*- coding: utf-8 -*-
"""labelling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBd0EatRPnllAkd4mFDCeBQTnXN2zfHx
"""

!pip install transformers datasets  scikit-learn tqdm

!pip uninstall -y torch torchvision torchaudio

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

!pip install transformers

import torch
import torchvision
print(torch.__version__)
print(torchvision.__version__)

import sys
print(sys.version)

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Set CPU threading for better performance
torch.set_num_threads(8)

# Load first model: RoBERTa-based hate speech classifier
model_1_name = "cardiffnlp/twitter-roberta-base-offensive"
tokenizer_1 = AutoTokenizer.from_pretrained(model_1_name)
model_1 = AutoModelForSequenceClassification.from_pretrained(model_1_name)
model_1.to("cuda")
model_1.eval()

# Load second model: DistilBERT sentiment classifier
model_2_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer_2 = AutoTokenizer.from_pretrained(model_2_name)
model_2 = AutoModelForSequenceClassification.from_pretrained(model_2_name)
model_2.to("cuda")
model_2.eval()

# Load dataset
file_path = "/content/drive/My Drive/TW/Dataset/collection/cleaned_translated_dataset.csv"  # Update path if needed
df = pd.read_csv(file_path)

# Batch processing function
def classify_batch(texts, batch_size=16):
    labels = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]

        # Process input for Model 1 (Hate Speech)
        inputs_1 = tokenizer_1(batch_texts, return_tensors="pt", truncation=True, padding=True, max_length=512)
        inputs_1 = {key: val.to("cuda") for key, val in inputs_1.items()}

        # Process input for Model 2 (Sentiment)
        inputs_2 = tokenizer_2(batch_texts, return_tensors="pt", truncation=True, padding=True, max_length=512)
        inputs_2 = {key: val.to("cuda") for key, val in inputs_2.items()}

        with torch.no_grad():
            logits_1 = model_1(**inputs_1).logits
            logits_2 = model_2(**inputs_2).logits

        predictions_1 = torch.argmax(logits_1, dim=1).tolist()  # 0 = Not Offensive, 1 = Offensive
        predictions_2 = torch.argmax(logits_2, dim=1).tolist()  # 0 = Negative, 1 = Positive

        for p1, p2 in zip(predictions_1, predictions_2):
            if p1 == 1 and p2 == 0:
                labels.append("bully")
            elif p1 == 0 and p2 == 1:
                labels.append("not bully")
            else:
                labels.append("unsure")

    return labels

# Apply batch classification
df["Label"] = classify_batch(df["Translated_Text"].tolist())

df['Label'].value_counts().to_dict()

# Separate "unsure" labeled rows
df_unsure = df[df["Label"] == "unsure"]

# Save "unsure" labeled data to a separate file
df_unsure.to_csv("/content/drive/My Drive/TW/Dataset/collection/unsure_labeled_tweets.csv", index=False)
print("✅ Unsure labeled dataset saved as unsure_labeled_tweets.csv")

# Keep only "bully" and "not bully" in the main dataset
df_filtered = df[df["Label"] != "unsure"]

# Save the filtered dataset
df_filtered.to_csv("/content/drive/My Drive/TW/Dataset/collection/filtered_labeled_tweets.csv", index=False)
print("✅ Filtered dataset (without 'unsure' labels) saved as filtered_labeled_tweets.csv")

"""**Fine-Tuning BERT for Cyberbullying Detection**"""

import torch
import pandas as pd
import numpy as np
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.model_selection import train_test_split
from torch.nn import functional as F

# Set device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load labeled dataset
file_path = "bert_labeled_dataset.csv"
df = pd.read_csv(file_path)

# Encode labels (convert "bully" → 1, "not bully" → 0)
df["Label"] = df["Label"].map({"bully": 1, "not bully": 0})

# Load tokenizer
model_name = "bert-base-uncased"  # You can also use "bertweet-base"
tokenizer = BertTokenizer.from_pretrained(model_name)

# Tokenization function
class CyberbullyingDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, index):
        text = self.texts[index]
        label = self.labels[index]
        encoding = self.tokenizer(text, padding="max_length", truncation=True, max_length=self.max_len, return_tensors="pt")

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "label": torch.tensor(label, dtype=torch.long),
        }

# Split data into train and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(df["Translated_Text"], df["Label"], test_size=0.2, random_state=42)

# Create dataset objects
train_dataset = CyberbullyingDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)
val_dataset = CyberbullyingDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)

# Create DataLoader
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Load pre-trained BERT model
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)
model.to(device)

# Optimizer & Loss Function
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)
criterion = torch.nn.CrossEntropyLoss()

# Training Loop
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids, attention_mask, labels = batch["input_ids"].to(device), batch["attention_mask"].to(device), batch["label"].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask)
        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_train_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}")

# Save the fine-tuned model
model.save_pretrained("fine_tuned_bert_model")
tokenizer.save_pretrained("fine_tuned_bert_model")

print("Fine-tuning completed! Model saved as 'fine_tuned_bert_model'.")

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# ---------------- EVALUATION ---------------- #

def evaluate_model(model, val_loader):
    model.eval()
    predictions, true_labels = [], []

    with torch.no_grad():
        for batch in val_loader:
            input_ids, attention_mask, labels = batch["input_ids"].to(device), batch["attention_mask"].to(device), batch["label"].to(device)
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            labels = labels.cpu().numpy()

            predictions.extend(preds)
            true_labels.extend(labels)

    accuracy = accuracy_score(true_labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average="binary")
    conf_matrix = confusion_matrix(true_labels, predictions)

    print("\n--- Model Evaluation ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")

    # Plot Confusion Matrix
    plt.figure(figsize=(5, 5))
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Bully", "Bully"], yticklabels=["Not Bully", "Bully"])
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.title("Confusion Matrix")
    plt.show()

# Call evaluation function
evaluate_model(model, val_loader)
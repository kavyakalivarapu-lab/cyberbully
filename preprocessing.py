# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H4_7n7p0AvxTWHkqr30QzLMA9G9x6ULe
"""

import torch

# Check if GPU is available
print("GPU Available:", torch.cuda.is_available())

# Print GPU details
if torch.cuda.is_available():
    print("GPU Name:", torch.cuda.get_device_name(0))
    print("GPU Count:", torch.cuda.device_count())

import tensorflow as tf

# Check if TensorFlow is using GPU
print("TensorFlow GPU:", tf.config.list_physical_devices('GPU'))

with tf.device('/GPU:0'):
    tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])
    print("Tensor on GPU:", tensor)

import torch

# Move a tensor to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tensor = torch.tensor([1.0, 2.0, 3.0]).to(device)
print("Tensor on GPU:", tensor)

# Move tensor to GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tensor = torch.rand(3, 3).to(device)

print("Tensor on:", device)  # It should print "cuda"

!pip install requests beautifulsoup4 emoji langdetect deep-translator googletrans

import pandas as pd
import re
import emoji
import requests
import spacy
from langdetect import detect
from googletrans import Translator
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

# Load dataset (update the file path as needed)
file_path = "/content/drive/My Drive/TW/Dataset/final_merged_tweets.csv"  # Change this to your actual dataset path
df = pd.read_csv(file_path)

len(df)

# Remove duplicate rows
df.drop_duplicates(inplace=True)

len(df)

# Load spaCy model for Named Entity Recognition (NER)
nlp = spacy.load("en_core_web_sm")

# Function to remove @mentions
def remove_mentions(text):
    return re.sub(r"@\w+", "", text)  # Remove words starting with '@'

# Function to remove named entities
def remove_named_entities(text):
    doc = nlp(text)
    return " ".join([token.text for token in doc if not token.ent_type_])  # Keep non-entities

# Apply Named Entity Removal
df["Processed_Text"] = df["Text"].apply(remove_mentions)

# Apply Named Entity Removal
df["Processed_Text"] = df["Processed_Text"].apply(remove_named_entities)

df["Processed_Text"].head(10)

# Expanded emoji mapping for better translations
emoji_dict = {
    "thinking_face": "confused",
    "angry_face": "angry",
    "grinning_face": "happy",
    "crying_face": "sad",
    "thumbs_up": "good",
    "thumbs_down": "bad",
    "face_with_tears_of_joy": "laughing",
    "red_heart": "love",
    "fire": "amazing",
    "clapping_hands": "applause",
    "smiling_face_with_sunglasses": "cool",
    "rolling_on_the_floor_laughing": "hilarious",
    "face_with_steam_from_nose": "frustrated",
    "loudly_crying_face": "heartbroken",
    "folded_hands": "please",
    "hundred_points": "perfect",
    "party_popper": "celebration",
    "exploding_head": "mind-blown",
    "face_screaming_in_fear": "terrified",
    "face_with_monocle": "curious",
    "face_vomiting": "disgusted",
    "sleeping_face": "sleepy",
    "nauseated_face": "sick",
    "dizzy_face": "shocked",
    "smiling_face_with_hearts": "in love",
    "shushing_face": "quiet",
    "winking_face": "playful",
    "neutral_face": "meh",
    "pouting_face": "annoyed",
    "money_bag": "rich",
    "pile_of_poo": "nonsense",
}

# Convert emojis to text with improved mapping
def convert_emojis(text):
    text = emoji.demojize(text, delimiters=(" ", " "))  # Convert emojis to words
    for emoj in emoji_dict:
        text = text.replace(emoj, emoji_dict[emoj])  # Replace with better words
    return text

df["Processed_Text"] = df["Processed_Text"].astype(str).apply(convert_emojis)

df['Processed_Text'].head(10)

# Extract URLs and place in a new column
def extract_urls(text):
    urls = re.findall(r'http[s]?://\S+|www\.\S+', text)
    return urls[0] if urls else None

df["URL"] = df["Text"].apply(extract_urls)
df["Processed_Text"] = df["Processed_Text"].apply(lambda x: re.sub(r'http[s]?://\S+|www\.\S+', '', x))

# Remove retweet indicators
df['Processed_Text'] = df['Processed_Text'].str.replace(r'\bRT\b', '', regex=True)

# Convert byte strings to normal strings
df['Processed_Text'] = df['Processed_Text'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

df['Processed_Text'].tail(20)

# Detect language and translate if necessary
translator = Translator()

languages_to_translate = {"id": "Indonesian", "ja": "Japanese", "hi": "Romanized Hindi", "te": "Telugu", "fr": "French"}

def detect_and_translate(text):
    try:
        detected_lang = detect(text)
        if detected_lang in languages_to_translate and detected_lang != "en":
            translated_text = translator.translate(text, dest="en").text
            return translated_text
        return text  # Return original if already in English
    except:
        return None  # Remove if processing fails

with ThreadPoolExecutor() as executor:
    df["Processed_Text"] = list(executor.map(detect_and_translate, df["Processed_Text"]))

df['Processed_Text'].iloc[0]

import unicodedata

def clean_text(text):
    return unicodedata.normalize("NFKC", text)

text = "yg bisa poster / carousel post tema cyberbullying ? format , deadline kamis depan pls info # zonauang \n zonabaᅠᅠᅠ"
cleaned_text = clean_text(text)
print(cleaned_text)

# Find rows where 'Text' is the same as 'Processed_Text'
identical_rows = df[df["Text"] == df["Processed_Text"]]

# Get row indices
identical_row_indices = identical_rows.index.tolist()

# Print results
print(f"Number of rows where 'Text' == 'Processed_Text': {len(identical_row_indices)}")
print("Row numbers of unprocessed rows:", identical_row_indices)

# (Optional) Save identical rows for debugging
identical_rows.to_csv("identical_rows.csv", index=False)

# Remove these rows from the dataset
df = df[df["Text"] != df["Processed_Text"]]

# Save the preprocessed dataset
df.to_csv("/content/drive/My Drive/TW/Dataset/collection/preprocessed_dataset.csv", index=False)

print(f"Final dataset has {len(df)} rows after removing identical rows.")

# Count values that are either NaN, empty strings, or only whitespace
num_empty = df["Processed_Text"].apply(lambda x: isinstance(x, str) and x.strip() == "").sum()

# Check for non-standard empty values (e.g., 'nan', 'None', etc.)
num_non_standard = df["Processed_Text"].isin(["nan", "None", "NULL"]).sum()

# Print results
print(f"Number of empty string values: {num_empty}")
print(f"Number of non-standard empty values: {num_non_standard}")
print(f"Total number of empty values: {num_empty + num_non_standard}")

print(df[df["Processed_Text"].apply(lambda x: isinstance(x, str) and x.strip() == "")])

# Remove rows where 'Processed_Text' is NaN or contains non-standard empty values
df_cleaned = df.dropna(subset=["Processed_Text"])  # Remove NaN values
df_cleaned = df_cleaned[~df_cleaned["Processed_Text"].astype(str).isin(["nan", "None", "NULL"])]

# Save the cleaned dataset
df_cleaned.to_csv("/content/drive/My Drive/TW/Dataset/collection/cleaned_dataset.csv", index=False)

print(f"Rows removed: {len(df) - len(df_cleaned)}")
print("Cleaned dataset saved as 'cleaned_dataset.csv'")

from langdetect import detect, DetectorFactory
import pandas as pd

# Ensure language detection is consistent
DetectorFactory.seed = 0

# Function to detect language
def detect_language(text):
    try:
        return detect(text) if isinstance(text, str) and text.strip() else "unknown"
    except:
        return "unknown"

# Apply language detection to 'Processed_Text' column
df["Detected_Lang"] = df["Processed_Text"].apply(detect_language)

# Count non-English rows
num_non_english = (df["Detected_Lang"] != "en").sum()

print(f"Number of non-English rows in 'Processed_Text': {num_non_english}")

import pandas as pd
from langdetect import detect, DetectorFactory

# Ensure language detection is consistent
DetectorFactory.seed = 0

# Function to detect language
def detect_language(text):
    try:
        return detect(text) if isinstance(text, str) and text.strip() else "unknown"
    except:
        return "unknown"


# Detect language
df["Detected_Lang"] = df["Processed_Text"].apply(detect_language)

# Remove non-English rows
df_filtered = df[df["Detected_Lang"] == "en"]

# Save the cleaned dataset
df_filtered.to_csv("/content/drive/My Drive/TW/Dataset/collection/cleaned_dataset.csv", index=False)

print(f"Removed {num_non_english} non-English rows. Cleaned dataset saved as 'cleaned_dataset.csv'.")

!pip install langid

import langid

# Function to detect language using langid
def detect_language(text):
    if isinstance(text, str) and text.strip():
        return langid.classify(text)[0]  # Returns the language code
    return "unknown"

# Apply language detection
df["Detected_Lang"] = df["Processed_Text"].apply(detect_language)

# Count non-English rows
num_non_english = (df["Detected_Lang"] != "en").sum()

print(f"Number of non-English rows in 'Processed_Text': {num_non_english}")

df.head()

from deep_translator import GoogleTranslator
import pandas as pd

# Initialize Google Translator
translator = GoogleTranslator(source="auto", target="en")

# Function to translate non-English text
def translate_to_english(text, lang):
    if pd.isna(text) or pd.isna(lang):  # Handle missing values
        return "missing_text"

    text = str(text).strip()  # Convert to string and strip spaces

    if lang == "en" or text == "":
        return text  # Keep English text unchanged

    try:
        return translator.translate(text)
    except:
        return "translation_failed"



# Apply translation only to non-English rows
df["Translated_Text"] = df.apply(lambda row: translate_to_english(row["Processed_Text"], row["Detected_Lang"]), axis=1)

# Save translated dataset
df.to_csv("/content/drive/My Drive/TW/Dataset/collection/translated_dataset.csv", index=False)

print("Translation completed and saved as 'translated_dataset.csv'.")

